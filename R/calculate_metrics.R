#' Calculation of tracking metrics.
#'
#' Calculates metrics from cage location data.
#'
#' This function takes the pre-processed data from \code{\link{read_data}} and
#' calculates a range of behavioural metrics. Metrics are calculated daily, with
#' the exception of the ethogram data which are calculated hourly.
#'
#' @param data A \code{colonytrack_data} object, generated by
#'   \code{\link{read_data}}.
#' @param days Specify the days to be included is the metrics calculation. This
#'   may be in any of the standard formats (names, indices, or a logical
#'   vector). Default is to include all days present in the input data file.
#' @param drop.days Specify the days to be _excluded_ is the metrics
#'   calculation. This takes precedence over the \code{days} parameter. This may
#'   be in any of the standard formats (names, indices, or a logical vector).
#'   Default is not to exclude any days.
#' @param subjects Specify the subjects to be included is the metrics
#'   calculation. This may be in any of the standard formats (names, indices, or
#'   a logical vector). Default is to include all subjects present in the input
#'   data file.
#' @param drop.subjects Specify the subjects to be _excluded_ is the metrics
#'   calculation. This takes precedence over the \code{subjects} parameter. This
#'   may be in any of the standard formats (names, indices, or a logical
#'   vector). Default is not to exclude any subjects.
#' @param trim An integer vector of length 2, specifying the number of hours (in
#'   Zeitgeber time) to be trimmed off the ends of the metrics calculation
#'   period. This may be useful to allow focus on a core segment of nightly
#'   activity. Default is no trimming.
#' @param estimate Logical (default \code{FALSE}) Whether an estimate of total
#'   runtime should be shown as the calculation proceeds. The calculation of
#'   metrics on a large dataset can take many hours, so thus can be a useful
#'   indicator. Performance is, however, better when this option is set to
#'   false.
#' @param log Logical (default \code{FALSE}) indicating whether a file should be
#'   created for each job. This is useful for monitoring large experiments and
#'   for estimating total run time. Note that a progress bar is not provided as
#'   the computational overhead is too high.
#' @param n.cores The number of CPU cores to use. The default tries to estimate
#'   an optimal value but, especially on Linux, it may be better to set this
#'   manually based on the available hardware. If \code{n.cores} is set higher
#'   than the number of days to be calculated, then the cluster will be created
#'   with only one node per day.
#'
#' @return An \code{colonytrack_metrics} object. The \code{calls} element
#'   contains the called strategy/strategies as well as confidence scores for
#'   all possible strategies.
#'
#' @seealso \code{\link{read_data}}.
#'
#' @importFrom stats setNames na.omit median sd
#' @importFrom utils head packageVersion
#' @importFrom pbapply pboptions pblapply
#' @importFrom parallel makeForkCluster detectCores stopCluster
#'
#' @export
calculate_metrics = function(data, days = "all", drop.days = NULL, subjects = "all", drop.subjects = NULL, trim = c(0, 0), estimate = FALSE, log = FALSE, n.cores = parallel::detectCores() - 1){
	start.time = Sys.time()
	window.definitions = NULL
	if(is.numeric(days) | is.logical(days)) days = data$info$days$id[days]
	if(is.numeric(drop.days)) drop.days = data$info$days$id[drop.days]
	if(any(class(days) == "colonytrack_windows")){
		window.definitions = days
	}else if(days[1] == "all"){
		days = setdiff(data$info$days$id, drop.days)
	}else{
		if(any(grepl("\u2010|\u2011|\u2212|\u2013|\u2014", days))) message("It looks like one of the dates might use a non-hyphen dash. The dates should only use 'hyphen-minus' characters as separators.")
		if(!all(days %in% data$info$days$id)) stop("The 'days' parameter must only specify days present in the data file.")
	}
	if(is.numeric(subjects) | is.logical(subjects)) subjects = names(data$data)[subjects]
	if(is.numeric(drop.subjects)) drop.subjects = names(data$data)[drop.subjects]
	if(subjects[1] == "all"){
		subjects = names(data$data)
	}else{
		if(!all(subjects %in% names(data$data))) stop("The 'subjects' parameter must only specify subjects present in the data file.")
	}
	subjects = setdiff(subjects, drop.subjects)
	subject.info = data$info$subject.info[match(subjects, data$info$subject.info$SubjectID), ]

	trim.numeric = as.numeric(trim)
	trim = as.integer(trim)
	if(sum(trim - trim.numeric) != 0) stop("The 'trim' parameter must contain only integers.") # This is an error as rounded input could silently produce unexpected results.
	trim.sanity = TRUE
	if(any(trim < 0)) trim.sanity = FALSE
	if(any(trim >= 12)) trim.sanity = FALSE
	if(diff(trim * c(1, -1) + c(0, 12)) <= 0) trim.sanity = FALSE
	if(!trim.sanity) stop("The 'trim' parameter must specify a time block with the 12-hour analysis period.")


	# Firstly, the window indices are prepared, and then window-level metadata are processed in parallel.
	msg("Preparing data")
	# Window ids can be named anything, and must include a start and end in unix time.
	if(is.null(window.definitions)) window.definitions = data$info$nights[match(days, data$info$nights$id), ] # One window per night.
	class(window.definitions) = c("data.frame", "colonytrack_windows")
	window.definitions$start = window.definitions$start + (trim[1] * 3600)
	window.definitions$end = window.definitions$end - (trim[2] * 3600)

	window.data.list = apply(window.definitions, 1, function(window.definition){
		# Get subset of data for this window.
		window.start = as.numeric(window.definition["start"])
		window.end = as.numeric(window.definition["end"])
		this = list()
		this$window.definition = window.definition
		this$info = data$info
		this$info$subjects = this$info$subjects[subjects]
		this$layout = data$layout
		this$data = lapply(data$data[subjects], function(subject.data){
			# This odd IRanges implementation is over 20x faster than base subsetting.
			timestamp = na.omit(subject.data$Timestamp)
			dranges = IRanges::IRanges(start = timestamp, end = timestamp)
			tranges = IRanges::IRanges(start = window.start - 1, end = window.end + 1) # Add 1 s buffers as IRanges uses integer rounding.
			icol = subject.data[IRanges::from(IRanges::findOverlaps(dranges, tranges)), c("Timestamp", "Cage")] # Shorthand for the current subject/window data.
			previous = subject.data[rev(which(subject.data$Timestamp <= window.start))[1], c("Timestamp", "Cage")] # Previous cage in which subject was at the start of the window.
			previous$Timestamp = window.start
			rbind(previous, icol[which(icol$Timestamp >= window.start & icol$Timestamp < window.end), ]) # Refine to millisecond bounds.
		})
		return(this)
	}, simplify = FALSE)

	# This is exported for each window separately. This is to enable dynamic cage layouts and merging of metrics objects.
	cage.layout = lapply(seq_along(window.definitions$id), function(i) data$layout$graph)

	rm(data)
	invisible(gc())

	if(log){
		unlink("colonytrack.progress", recursive = T)
		dir.create("colonytrack.progress")
	}

	# Start the parallelised loop.
	msg("Calculating metrics")
	window.metrics = NULL

	if(estimate & nrow(window.definitions) > n.cores){
		cluster = parallel::makePSOCKcluster(min(n.cores, nrow(window.definitions)))
		first.batch.start.time = Sys.time()
		window.metrics.1 = parallel::parLapply(cluster, window.data.list[seq_along(cluster)], calculate_metrics_worker, log = log)
		first.batch.end.time = Sys.time()
		thread.rate = difftime(first.batch.end.time, first.batch.start.time, units = "auto")
		total.estimated.time = ceiling(nrow(window.definitions) / length(cluster)) * as.numeric(thread.rate, units = "secs")
		remaining.jobs = setdiff(seq_along(window.definitions$id), seq_along(cluster))
		estimated.time = total.estimated.time+first.batch.start.time-first.batch.start.time # Hack to reset the units to 'auto'.
		progress(paste("Estimated total time:", format(round(estimated.time, 2))))
		window.metrics = c(
			window.metrics.1,
			parallel::parLapply(cluster, window.data.list[remaining.jobs], calculate_metrics_worker, log = log)
		)
		parallel::stopCluster(cluster)
	}else{
		cluster = parallel::makePSOCKcluster(min(n.cores, nrow(window.definitions)))
		window.metrics = parallel::parLapply(cluster, window.data.list, calculate_metrics_worker, log = log)
		parallel::stopCluster(cluster)
	}

	msg("Collating results")
	features = lapply(window.metrics, function(window){
		window.data = do.call("rbind", lapply(window, function(w){
			if(is.null(w$individual)){
				return(get_na_defaults())
			}else{
				return(w$individual)
			}
		}))
		return(as.data.frame(window.data))
	})
	names(features) = window.definitions$id

	individual = lapply(features, function(f){
		individual.features = data.frame(
			distance.moved = f[, "path.length"],
			time.per.cage = f[, "mean.time.in.cage"],
			high.activity = f[, "time.high.activity"],
			sustained.activity = f[, "time.sustained.activity"],
			cage.variability = f[, "sd.cage.use"],
			cage.time.entropy = f[, "cage.time.entropy"],
			adjusted.cage.time.entropy = f[, "adjusted.cage.time.entropy"],
			cage.location.entropy = f[, "cage.location.entropy"],
			revisit.time = f[, "mean.revisit.time"],
			revisit.length = f[, "mean.revisit.length"],
			peak.inactive = f[, "peak.inactive.timepoint"],
			peak.active = f[, "peak.active.timepoint"],
			activity.blocks = f[, "number.activity.blocks"],
			cage.sharing = f[, "mean.cage.sharing"], # Mean number of animals in the same cage. Bounded to 11 animals/cage.
			time.alone = f[, "time.alone"],
			social.interaction = f[, "mean.social.interaction.time"], # Total _time_ spent with each other animal.
			social.distance = f[, "mean.distance.from.all"], # Mean distance from all others (in cage units).
			social.gradient = f[, "mean.sharing.change"], # Was the cage change to a more- or less-populated cage?
			social.influence = f[, "mean.influence"], # How many animals were in the cage in the 5 s after leaving it, vs. the number that were in the cage before and during occupancy?
			chase.events = f[, "number.chase.events"],
			chase.dominance = f[, "mean.chase.wins"]
		)
		rownames(individual.features) = rownames(f)
		return(individual.features)
	})
	names(individual) = window.definitions$id

	cage.probability = lapply(window.metrics, function(window){
		window.data = do.call("rbind", lapply(window, "[[", "cage.probability"))
		return(as.data.frame(window.data)[subjects, ])
	})
	names(cage.probability) = window.definitions$id

	ethogram = lapply(window.metrics, function(window){
		window.data = lapply(window, "[[", "ethogram")
		activity = t(sapply(window.data, function(m) m[, "activity"] ))
		exploration = t(sapply(window.data, function(m) m[, "exploration"] ))
		sociality = t(sapply(window.data, function(m) m[, "sociality"] ))
		return(list(activity = activity, exploration = exploration, sociality = sociality))
	})
	names(ethogram) = window.definitions$id

	# Clustering.
	interaction.time.clustering = lapply(window.metrics, function(window){
		interaction.time.metrics = sapply(lapply(window, "[[", "clustering"), "[[", "interaction.time")
		rownames(interaction.time.metrics) = colnames(interaction.time.metrics)
		# Make symmetrical regarding missing data.
		for(n in colnames(interaction.time.metrics)){
			missing = which(is.na(interaction.time.metrics[n, ]) | is.na(interaction.time.metrics[, n]))
			interaction.time.metrics[missing, n] = interaction.time.metrics[n, missing] = NA
		}
		return(interaction.time.metrics)
	})
	names(interaction.time.clustering) = window.definitions$id

	cage.share.clustering = lapply(window.metrics, function(window){
		cage.sharing.metrics = sapply(lapply(window, "[[", "clustering"), "[[", "cage.sharing")
		rownames(cage.sharing.metrics) = colnames(cage.sharing.metrics)
		# Make symmetrical regarding missing data.
		for(n in colnames(cage.sharing.metrics)){
			missing = which(is.na(cage.sharing.metrics[n, ]) | is.na(cage.sharing.metrics[, n]))
			cage.sharing.metrics[missing, n] = cage.sharing.metrics[n, missing] = NA
		}
		return(cage.sharing.metrics)
	})
	names(cage.share.clustering) = window.definitions$id

	social.distance.clustering = lapply(window.metrics, function(window){
		distance.metrics = sapply(lapply(window, "[[", "clustering"), "[[", "social.distance")
		rownames(distance.metrics) = colnames(distance.metrics)
		# Make symmetrical regarding missing data.
		for(n in colnames(distance.metrics)){
			missing = which(is.na(distance.metrics[n, ]) | is.na(distance.metrics[, n]))
			distance.metrics[missing, n] = distance.metrics[n, missing] = NA
		}
		return(distance.metrics)
	})
	names(social.distance.clustering) = window.definitions$id

	chase.count.clustering = lapply(window.metrics, function(window){
		distance.metrics = sapply(lapply(window, "[[", "clustering"), "[[", "chasing")
		rownames(distance.metrics) = colnames(distance.metrics)
		# Make symmetrical regarding missing data.
		for(n in colnames(distance.metrics)){
			missing = which(is.na(distance.metrics[n, ]) | is.na(distance.metrics[, n]))
			distance.metrics[missing, n] = distance.metrics[n, missing] = NA
		}
		return(distance.metrics)
	})
	names(chase.count.clustering) = window.definitions$id

	clustering = list(
		interaction.time = interaction.time.clustering,
		social.distance = social.distance.clustering,
		chasing = t(chase.count.clustering) # Note that this is now row-dominant (for compatibility with most other packages).
	)

	dominance = lapply(chase.count.clustering, function(network) colSums(sign(network - t(network)), na.rm = T) )

	chase.events = lapply(window.metrics, function(window){
		lapply(window, "[[", "chase.events")
	})
	names(chase.events) = window.definitions$id

	end.time = Sys.time()

	development = list(
		elapsed.time = as.numeric(end.time) - as.numeric(start.time),
		threads = lapply(window.metrics, function(window){
			list(
				start.time = sapply(window, function(w) w$dev$start.time),
				end.time = sapply(window, function(w) w$dev$end.time),
				elapsed.time = sapply(window, function(w) w$dev$elapsed.time)
			)
		})
	)

	rm(window.metrics)
	gc()

	metrics = list(
		info = list(
			windows = window.definitions,
			subjects = subjects,
			subject.info = subject.info,
			feature.names = names(get_defaults(NULL)),
			var.names = colnames(individual[[1]]),
			cage.layout = cage.layout,
			processed = Sys.time(),
			version = paste("ColonyTrack", packageVersion("ColonyTrack"))
		),
		features = features,
		individual = individual,
		cage.use = cage.probability,
		ethogram = ethogram,
		clustering = clustering,
		dominance = dominance,
		chase.events = chase.events,
		development = development
	)

	progress(paste("Total time taken:", format(round(difftime(end.time, start.time), 2), units = "auto")))

	class(metrics) = "colonytrack_metrics"

	return(metrics)
}
